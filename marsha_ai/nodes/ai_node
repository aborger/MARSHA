#!/usr/bin/env python3

import gym
import rospy
import numpy as np

from stable_baselines3 import TD3
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.callbacks import StopTrainingOnMaxEpisodes
from stable_baselines3.common.noise import NormalActionNoise

from marsha_ai.catch_bandit.gym_env import MarshaGym
from marsha_ai.catch_bandit.catch_interface import CatchInterface
from marsha_ai.callbacks import TensorboardCallback

import tensorflow as tf # so it doesnt have to initialize gpu later


def train():
    MODEL_DIR = rospy.get_param("/training_dirs/jet1/model_dir")
    LOG_DIR = rospy.get_param("/training_dirs/jet1/log_dir")

    interface = CatchInterface()
    env = MarshaGym(interface)

    callback_list = []
    callback_list.append(TensorboardCallback())
    #callback_list.append(StopTrainingOnMaxEpisodes(max_episodes=10, verbose=1))

    if rospy.get_param("/hyperparameters/eval_frequency") > 0: # Else not evaluating
        eval_callback = EvalCallback(env, best_model_save_path=MODEL_DIR, log_path=LOG_DIR,
                                    deterministic=True,
                                    eval_freq=rospy.get_param("/hyperparameters/eval_frequency"),
                                    n_eval_episodes=rospy.get_param('/hyperparameters/n_eval_ep'))
    callback_list.append(eval_callback)

    n_actions = env.action_space.shape[-1]
    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))

    #TODO: Setup model save checkpoints
    model = TD3("MlpPolicy", env, action_noise=action_noise, verbose=1, tensorboard_log=LOG_DIR, learning_starts=500) 
    #model = TD3.load(MODEL_DIR + 'best_model', env=env, tensorboard_log=LOG_DIR)
    model.learn(total_timesteps=rospy.get_param("/hyperparameters/total_timesteps"), callback=callback_list)

    model.save(MODEL_DIR + "TD3_Catch")

def run():
    MODEL_DIR = rospy.get_param("/training_dirs/jet1/model_dir")

    interface = CatchInterface()
    env = MarshaGym(interface)


    model = TD3.load(MODEL_DIR + 'best_model', env=env)
    
    obs = env.reset()
    while True:
        action, states = model.predict(obs)
        obs, rewards, dones, info = env.step(action)

if __name__ == "__main__":
    train()
    print("Done.")