#!/usr/bin/env python

# Use to determine the precision and accuracy of the object location measurement

# Author: Aaron Borger

import rospy
import numpy as np

import math

from geometry_msgs.msg import Point

class Measurer:
    def __init__(self):
        rospy.init_node('measurer')

        obj_sub = rospy.Subscriber("object_pos", Point, self.obj_pos_cb)

        self.measurements = []


    def obj_pos_cb(self, msg):
        print(msg)
        self.measurements.append([msg.x, msg.y, msg.z])
    
    def run(self):
        while not rospy.is_shutdown():
            rospy.spin()
        sum = np.zeros((3,))
        for pos in self.measurements:
            m = np.array(pos)
            sum += m
        avg = sum / len(self.measurements)
        print("avg:", avg)

        square_sum = 0
        for pos in self.measurements:
            dist = math.sqrt((pos[0] - avg[0])**2 + (pos[1] - avg[1])**2 + (pos[2] - avg[2])**2)
            square_sum += dist**2
        
        variance = square_sum / len(self.measurements)
        standard_dev = math.sqrt(variance)
        print("standard deviation: " + str(standard_dev))




if __name__ == "__main__":
    m = Measurer()
    #try:
    m.run()
    #except KeyboardInterrupt:

    